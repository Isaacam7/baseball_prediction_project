{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#web scraping\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#regular expressions\n",
    "import re\n",
    "    \n",
    "#for saving workdata\n",
    "import pickle as pk\n",
    "\n",
    "#in this case, numpy for nan\n",
    "import numpy as np\n",
    "\n",
    "#for time\n",
    "import datetime as dt\n",
    "\n",
    "#for lagging requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for slacing from a pattern, provided by ChatGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_from_pattern(string, pattern):\n",
    "    # Use the `search` function to find the position of the pattern in the string\n",
    "    match = re.search(pattern, string)\n",
    "    if match:\n",
    "        # Get the start index of the match\n",
    "        start_index = match.start()\n",
    "        # Return the slice of the string from the start index to the end\n",
    "        return string[start_index:]\n",
    "    # If the pattern is not found, return the original string\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start off by pulling the boxscore links for all games so each individual boxscore can be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#start and end year is inclusive\n",
    "def get_box_links(start_year,end_year):\n",
    "    box_links = []\n",
    "    for year in range(start_year,end_year + 1):\n",
    "        #page to scrape\n",
    "        url = f\"https://www.baseball-reference.com/leagues/majors/{year}-schedule.shtml\"\n",
    "        resp = req.get(url)\n",
    "        soup = bs(resp.text)\n",
    "\n",
    "        \"\"\"\n",
    "           this list comprehhension finds all instances of a tags with the text being boxscores, \n",
    "           and for each instance it only takes the href attribute or unique boxscore link.\n",
    "           \n",
    "           the extending adds all elements of the resultant list to the box_links list\n",
    "        \"\"\"\n",
    "        box_links.extend([x[\"href\"] for x in soup.find_all(\"a\",text=\"Boxscore\")])\n",
    "    return box_links\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "head over to the site to see the layout, use inspect element to see the html as well to know how to get the boxscore links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_links = get_box_links(2016,2022)\n",
    "pk.dump(box_links, open('box_links.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_links = pk.load(open('box_links.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now all the boxscore links from 2016-2022 (inclusive) are obtained, the scraping for specific stats can begin, start by creating a function to manage collecting data from the different tables and creating the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_organizer(box_link):\n",
    "    \n",
    "    box_url = \"http://baseball-reference.com\" + box_link  \n",
    "    resp = req.get(box_url)\n",
    "    soup = bs(resp.text)\n",
    "    \n",
    "    #isolate the specific game id from the box link\n",
    "    box_id = box_link.split('/')[-1][:-6]\n",
    "    \n",
    "    #TODO: pass on id and soup to other funcs to create the other dicts for tables\n",
    "    \n",
    "    #get rid of weird comments found in the html, this code block from: rdoharr's MLB prediction project, thanks\n",
    "    uncommented_html = ''\n",
    "    for h in resp.text.split('\\n'):\n",
    "        if '<!--     <div' in h: continue\n",
    "        if h.strip() == '<!--': continue\n",
    "        if h.strip() == '-->': continue\n",
    "        uncommented_html += h + '\\n'\n",
    "    \n",
    "    #the order which the tables appear is, away_batting, home_batting, away_pitching, home pitching, (the order place, is also the table number)\n",
    "    \n",
    "    uncommented_soup = bs(uncommented_html)\n",
    "    gamedata = {\n",
    "        \"general_game_data\" : get_game_facts(soup,box_id),\n",
    "        \"away_batting_data\" : get_table_summary(uncommented_soup,0),\n",
    "        \"home_batting_data\" : get_table_summary(uncommented_soup,1),\n",
    "        \"away_pitching_data\" : get_table_summary(uncommented_soup,2),\n",
    "        \"home_pitching_data\" : get_table_summary(uncommented_soup,3),\n",
    "        \"away_pitcher_data\" : get_pitcher_table(uncommented_soup,0),\n",
    "        \"home_pitcher_data\" : get_pitcher_table(uncommented_soup,1)\n",
    "    }\n",
    "    \n",
    "    return gamedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first I want a dict that will include:  \n",
    "general facts about the game such as, which teams played, weather, time, ect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_game_facts(soup,box_id):\n",
    "    game_facts = {\"game_id\":box_id}\n",
    "    #find the scorebox section that includes date, attendance, ect\n",
    "    scorebox = soup.find(\"div\",class_=\"scorebox\")\n",
    "        \n",
    "    #this list comprehension goes through all \"a\" tags in the  scorebox section and only  adds each tag if \"teams\" is in the href\n",
    "\n",
    "    names = [x for x in scorebox.find_all(\"a\") if \"teams\" in x[\"href\"]]\n",
    "\n",
    "    #for each tag, isolate thhe href, and then isoolate the team abbreviation \n",
    "    game_facts[\"away_team_abr\"] = names[0][\"href\"].split('/')[2]\n",
    "    game_facts[\"home_team_abr\"] = names[1][\"href\"].split('/')[2]\n",
    "\n",
    "    div_data = scorebox.find(\"div\",class_ = \"scorebox_meta\").find_all(\"div\")\n",
    "\n",
    "    #not going to explain simple list compreheensions anymore since there are already enough exps...\n",
    "    otherdata = [x.text for x in div_data if \"Rescheduled\" not in x.text][:-1]\n",
    "    \n",
    "    #date always the first thing\n",
    "    game_facts[\"date\"] = otherdata[0]\n",
    "         \n",
    "    flag = False\n",
    "    index = 0\n",
    "    \n",
    "    for i in range(len(otherdata)):\n",
    "        if \"Start\" in otherdata[i]:\n",
    "            index = i\n",
    "            flag = True\n",
    "    \n",
    "    \n",
    "    game_facts[\"start_time\"] = otherdata[index][12:-6]\n",
    "    flag = False\n",
    "\n",
    "    \n",
    "    for i in range(len(otherdata)):\n",
    "        #this loop format will check if data contains unique information\n",
    "        if \"Attendance\" in otherdata[i]:\n",
    "            flag = True\n",
    "            index = i\n",
    "            break\n",
    "    if flag == True:      \n",
    "        game_facts[\"attendance\"] = otherdata[index][12:].replace(',','')\n",
    "    else:\n",
    "        game_facts[\"attendance\"] = np.nan\n",
    "        \n",
    "\n",
    "\n",
    "    flag = False\n",
    "    \n",
    "    \n",
    "   \n",
    "    for i in range(len(otherdata)):\n",
    "        if \"Venue\" in otherdata[i]:\n",
    "            flag = True\n",
    "            index = i\n",
    "            break\n",
    "            \n",
    "    if flag == True:\n",
    "        game_facts[\"venue\"] = otherdata[index][7:]\n",
    "    else:\n",
    "        game_facts[\"venue\"] = np.nan\n",
    "        \n",
    "\n",
    "    flag = False\n",
    "    \n",
    "    for i in range(len(otherdata)):\n",
    "        if \"Duration\" in otherdata[i]:\n",
    "            flag = True\n",
    "            index = i\n",
    "            break\n",
    "    \n",
    "    if flag == True:\n",
    "        game_facts[\"duration_hours\"] = int(otherdata[index][15:-3]) + (int(otherdata[index][17:]) / 60)\n",
    "        game_facts[\"duration_mins\"] = (int(otherdata[index][15:-3]) * 60) + int(otherdata[index][17:])\n",
    "    else:\n",
    "        game_facts[\"duration_hours\"] = np.nan\n",
    "        game_facts[\"duration_mins\"] = np.nan\n",
    "    \n",
    "\n",
    "    flag = False\n",
    "    \n",
    "    \n",
    "    day_grass_status = otherdata[0].split(', ')\n",
    "    \n",
    "    #ignore date\n",
    "    for i in range(len(otherdata)):\n",
    "        if i != 0 and \", \" in otherdata[i]:\n",
    "            index = i\n",
    "            flag = True\n",
    "            break\n",
    "    if flag == True:    \n",
    "        game_facts[\"day_game\"] = otherdata[index].split(', ')[0] == \"Day Game\"\n",
    "    else:\n",
    "        game_facts[\"day_game\"] = np.nan\n",
    "    \n",
    "    if flag == True:        \n",
    "        game_facts[\"on_grass\"] = otherdata[index].split(', ')[1] == \"on grass\"\n",
    "    else:\n",
    "        game_facts[\"on_grass\"] = np.nan\n",
    "        \n",
    "    flag = False\n",
    "\n",
    "    \"\"\"\n",
    "    find the div tag with class attribute, section_wrapper that contains the other info I am looking for.\n",
    "    Then get rid of the \\n and \\t characters.\n",
    "    Finally find all of the content that is between the strong and div tags\n",
    "    \"\"\"\n",
    "    rawotherinfo = re.findall(\"</strong>.*</div>\",str(soup.find_all(\"div\",class_=\"section_wrapper\")[2]).replace('\\n','').replace('\\t',''))\n",
    "\n",
    "    if rawotherinfo == []:\n",
    "        rawotherinfo = re.findall(\"</strong>.*</div>\",str(soup.find_all(\"div\",class_=\"section_wrapper\")[1]).replace('\\n','').replace('\\t',''))[0]\n",
    "    else:\n",
    "        rawotherinfo = rawotherinfo[0]\n",
    "\n",
    "    #get rid of the strong and div tags and then split based on sentences\n",
    "    otherinfo = rawotherinfo.replace(\"</div>\",'').replace(\"<div>\",'').replace(\"<strong>\",'').replace('</strong>','').split('.')\n",
    "    \n",
    "    #get rid of annoying arrow that pops up\n",
    "    if otherinfo[-1] == '    -->':\n",
    "        otherinfo = otherinfo[:-1]\n",
    "    \n",
    "    #since sometimes certain info is missing use flag to be safe\n",
    "    flag = False\n",
    "    \n",
    "    for i in range(len(otherinfo)):\n",
    "        #this loop format will check if data contains unique information\n",
    "        if \"HP\" in otherinfo[i]:\n",
    "            flag = True\n",
    "            index = i\n",
    "            break\n",
    "    if flag == True:\n",
    "        umpires = otherinfo[index].split(',')\n",
    "        try:\n",
    "            #terenary operator to condense if statement, in the format of: [return_val] if [condition] else [statement_on_false\n",
    "            game_facts[\"hp_umpire\"] = umpires[0][6:] if \"HP\" in umpires[0] else np.nan\n",
    "            \n",
    "        except:\n",
    "             game_facts[\"hp_umpire\"] = np.nan\n",
    "                \n",
    "        try:\n",
    "            game_facts[\"1b_umpire\"] = umpires[1][6:] if \"1B\" in umpires[1] else np.nan\n",
    "            \n",
    "        except:\n",
    "             game_facts[\"1b_umpire\"] = np.nan\n",
    "        \n",
    "        try:\n",
    "            game_facts[\"2b_umpire\"] = umpires[2][6:] if \"2B\" in umpires[2] else np.nan\n",
    "            \n",
    "        except:\n",
    "             game_facts[\"2b_umpire\"] = np.nan\n",
    "                \n",
    "        try:\n",
    "            game_facts[\"3b_umpire\"] = umpires[3][6:] if \"3B\" in umpires[3] else np.nan\n",
    "            \n",
    "        except:\n",
    "             game_facts[\"3b_umpire\"] = np.nan\n",
    "                \n",
    "    else:\n",
    "        game_facts[\"hp_umpire\"] = np.nan\n",
    "        game_facts[\"1b_umpire\"] = np.nan\n",
    "        game_facts[\"2b_umpire\"] = np.nan\n",
    "        game_facts[\"3b_umpire\"] = np.nan\n",
    "        \n",
    "\n",
    "    flag = False\n",
    "    \n",
    "    for i in range(len(otherinfo)):\n",
    "        if \"Field\" in otherinfo[i]:\n",
    "            flag = True\n",
    "            index = i\n",
    "            break\n",
    "    \n",
    "    if flag == True:\n",
    "        game_facts[\"field_cond\"] = otherinfo[index][17:]\n",
    "    else:\n",
    "        game_facts[\"field_cond\"] = np.nan\n",
    "        \n",
    "\n",
    "    flag = False\n",
    "    \n",
    "    \n",
    "    for i in range(len(otherinfo)):\n",
    "        if \"Start Time Weather\" in otherinfo[i]:\n",
    "            flag = True\n",
    "            index = i\n",
    "            break\n",
    "  \n",
    "            \n",
    "    if flag == True:       \n",
    "        conditions = otherinfo[i].split(',')\n",
    "    else:\n",
    "        game_facts[\"start_weather\"] = np.nan\n",
    "        game_facts[\"wind_cond\"] = np.nan\n",
    "        game_facts[\"sky_cond\"] = np.nan\n",
    "        return game_facts\n",
    "    \n",
    "    flag = False\n",
    "   \n",
    "    weather = conditions[0][20:].replace('&deg','').split('; ')\n",
    "\n",
    "    #just in case baseball reference uploads in celcius instead of farenheit\n",
    "    if len(weather) == 2 and (weather[1] == 'C' or weather[1] == 'c'):\n",
    "        try: \n",
    "            weather[0] = (weather[0] * 1.8) + 32\n",
    "        except:\n",
    "            weather[0] = np.nan\n",
    "    \n",
    "    try:\n",
    "        weather[0] = float(weather[0])\n",
    "    except:\n",
    "        weather[0] = np.nan\n",
    "    \n",
    "    game_facts[\"start_weather\"] = weather[0]\n",
    "    \n",
    "    for i in range(len(conditions)):\n",
    "        if \"Wind\" in conditions[i]:\n",
    "            flag = True\n",
    "            index = i\n",
    "            break\n",
    "            \n",
    "    if flag == True:\n",
    "        game_facts[\"wind_cond\"] = conditions[index][1:]\n",
    "    else:\n",
    "        game_facts[\"wind_cond\"] = np.nan\n",
    "    \n",
    "    flag = False\n",
    "    \n",
    "    #if sky conditions were reported at all\n",
    "    if len(conditions) > 1 and \"Wind\" not in conditions[-1][1:]:\n",
    "        game_facts[\"sky_cond\"] = conditions[-1][1:]\n",
    "    else:\n",
    "        game_facts[\"sky_cond\"] = np.nan\n",
    "    return game_facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to make a function to pull the stats for batting/pitching, wether it is away or home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_summary(soup, table_num):\n",
    "    \n",
    "    table = {}\n",
    "    \n",
    "    #raw tables for batting and pitching for away/home \n",
    "    raw_stat_tables = soup.find_all(\"table\",class_=\"stats_table\")[1:][0:4]\n",
    "   \n",
    "\n",
    "    #specfic table to look for\n",
    "    raw_table = str(raw_stat_tables[table_num].find(\"tfoot\")).split('</td>')[:-1]\n",
    "    \n",
    "    \n",
    "    #get rid of the player data-stat\n",
    "    raw_table[0] = slice_from_pattern(raw_table[0].split('</th>')[1], \"data-stat\")\n",
    "\n",
    "    #loop through all other datastats and begin to add them\n",
    "    for stat in raw_table:\n",
    "        isolated_data_stat = slice_from_pattern(stat, \"data-stat\")\n",
    "        spliced_data_stat = isolated_data_stat[10:].split(\">\")\n",
    "\n",
    "        data_name = spliced_data_stat[0].replace('\"', '')\n",
    "        data_val = spliced_data_stat[1].replace(' ', '').replace(\"%\",'')\n",
    "        \n",
    "        table[data_name] = data_val\n",
    "        \n",
    "    keys = list(table.keys())\n",
    "    if 'details' in keys:\n",
    "        del table['details']\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I want individual data for each of the teams starting pitchers in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw tables for batting and pitching for away/home\n",
    "def get_pitcher_table(soup, table_num):\n",
    "    raw_stat_tables = soup.find_all(\"table\",class_=\"stats_table\")[1:][2:4]\n",
    "\n",
    "    starting_pitcher = raw_stat_tables[table_num].find(\"tbody\").find(\"th\")[\"data-append-csv\"]\n",
    "    table = {'starting_pitcher' : starting_pitcher}\n",
    "\n",
    "    raw_table = str(raw_stat_tables[table_num].find(\"tbody\")).split('</td>')\n",
    "\n",
    "    raw_table[0] = slice_from_pattern(raw_table[0].split('</th>')[1], \"data-stat\")\n",
    "    raw_tables = raw_table[:26]\n",
    "\n",
    "    for stat in raw_tables:\n",
    "        isolated_data_stat = slice_from_pattern(stat, \"data-stat\")\n",
    "        spliced_data_stat = isolated_data_stat[10:].split(\">\")\n",
    "\n",
    "        data_name = spliced_data_stat[0].replace('\"', '')\n",
    "        data_val = spliced_data_stat[1].replace(' ', '').replace(\"%\",'')\n",
    "        \n",
    "        table[data_name] = data_val\n",
    "        \n",
    "    del table[\"inherited_runners\"]\n",
    "    del table[\"inherited_score\"]\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the functions for creating a game dictionary is finished, the list of games to store each dictionary can be constructed  \n",
    "this is what will be used to build the games dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_list_maker():\n",
    "    print(f\"current time is: {dt.datetime.now()}\")\n",
    "    startind = len(game_list)\n",
    "    print(f\"starting at grabbing at page {startind}\")\n",
    "    \n",
    "    count = 1\n",
    "    \n",
    "    links = box_links[startind:]\n",
    "    for link in links:\n",
    "        if count % 20 == 0:\n",
    "            time.sleep(61)\n",
    "        \n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            gamedata = game_organizer(link)\n",
    "            #this will raise an error if none is returned\n",
    "            len(gamedata)\n",
    "            game_list.append(gamedata)\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(f\"stopped at the {len(game_list)} page, error was: {ex}\")\n",
    "            break\n",
    "        #time updates for game list making process\n",
    "        if len(game_list) % 1000 == 0:\n",
    "            print(f\"{len(game_list)} games have been processed, current time: {dt.datetime.now()}\")\n",
    "        if count % 50 == 0:\n",
    "            print(count)\n",
    "        count = count + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_list_maker()\n",
    "\n",
    "time.sleep(60)\n",
    "\n",
    "if len(game_list) < len(box_links):\n",
    "    time.sleep(300)\n",
    "    game_list_maker()\n",
    "    time.sleep(300)\n",
    "    \n",
    "if len(game_list) < len(box_links):\n",
    "    time.sleep(300)\n",
    "    game_list_maker()\n",
    "    \n",
    "if len(game_list) < len(box_links):\n",
    "    time.sleep(300)\n",
    "    time.sleep(300)\n",
    "    game_list_maker()\n",
    "    \n",
    "len(game_list)\n",
    "pk.dump(game_list, open('game_list.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the baseball-reference only allows 20 requests every minute and has pretty rough rate limiting procedures,  \n",
    "these time lags are needed to not get banned while pulling the data.  \n",
    "Expect to run this for more than a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.dump(game_list, open('game_list.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to game_list so we don't have to go through the requesting process all over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(box_links) == len(game_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all games were collected "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
